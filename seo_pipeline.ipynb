# SEO Content Quality & Duplicate Detector Pipeline
# Complete implementation for data science assignment

# ============================================================================
# SECTION 1: Setup and Imports
# ============================================================================

import pandas as pd
import numpy as np
import re
import json
import pickle
from pathlib import Path
from time import sleep
from typing import Dict, List, Tuple, Optional

# HTML parsing
from bs4 import BeautifulSoup
import requests

# Text processing
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import textstat

# ML and embeddings
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
from sentence_transformers import SentenceTransformer

# Download required NLTK data
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('punkt_tab', quiet=True)

# Create directories
Path('data').mkdir(exist_ok=True)
Path('models').mkdir(exist_ok=True)

print("✓ Setup complete!")

# ============================================================================
# SECTION 2: Data Collection & HTML Parsing (15%)
# ============================================================================

def parse_html_content(html_content: str) -> Dict[str, any]:
    """
    Parse HTML content and extract meaningful information.
    
    Args:
        html_content: Raw HTML string
        
    Returns:
        Dictionary with extracted title, body_text, and word_count
    """
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
        
        # Extract title
        title = soup.find('title')
        title = title.get_text().strip() if title else ""
        
        # Extract body text from main content areas
        body_text = ""
        
        # Priority selectors for main content
        content_selectors = ['article', 'main', '[role="main"]', '.content', '#content']
        
        for selector in content_selectors:
            content = soup.select_one(selector)
            if content:
                body_text = content.get_text(separator=' ', strip=True)
                break
        
        # Fallback to all paragraphs if no main content found
        if not body_text:
            paragraphs = soup.find_all('p')
            body_text = ' '.join([p.get_text(strip=True) for p in paragraphs])
        
        # Clean text
        body_text = re.sub(r'\s+', ' ', body_text).strip()
        
        # Calculate word count
        word_count = len(body_text.split()) if body_text else 0
        
        return {
            'title': title,
            'body_text': body_text,
            'word_count': word_count,
            'parse_success': True
        }
    
    except Exception as e:
        print(f"Error parsing HTML: {str(e)}")
        return {
            'title': "",
            'body_text': "",
            'word_count': 0,
            'parse_success': False
        }


def scrape_url(url: str) -> str:
    """
    Scrape HTML content from a URL (for alternative dataset or real-time analysis).
    
    Args:
        url: The URL to scrape
        
    Returns:
        HTML content as string
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        sleep(1)  # Rate limiting
        return response.text
    except Exception as e:
        print(f"Error scraping {url}: {str(e)}")
        return ""


def process_dataset(csv_path: str = 'data/data.csv') -> pd.DataFrame:
    """
    Process the input dataset and extract content from HTML.
    
    Args:
        csv_path: Path to input CSV file
        
    Returns:
        DataFrame with extracted content
    """
    # Read dataset
    df = pd.read_csv(csv_path)
    print(f"Loaded {len(df)} rows from dataset")
    
    # Check if html_content column exists (primary dataset)
    if 'html_content' in df.columns:
        print("Using primary dataset with HTML content")
        
        # Parse HTML content
        results = []
        for idx, row in df.iterrows():
            print(f"Parsing {idx+1}/{len(df)}: {row['url'][:60]}...")
            parsed = parse_html_content(row['html_content'])
            results.append({
                'url': row['url'],
                'title': parsed['title'],
                'body_text': parsed['body_text'],
                'word_count': parsed['word_count'],
                'parse_success': parsed['parse_success']
            })
        
        extracted_df = pd.DataFrame(results)
    
    else:
        # Alternative dataset - need to scrape
        print("Using alternative dataset (URLs only) - scraping required")
        
        results = []
        for idx, row in df.iterrows():
            print(f"Scraping {idx+1}/{len(df)}: {row['url'][:60]}...")
            html_content = scrape_url(row['url'])
            
            if html_content:
                parsed = parse_html_content(html_content)
                results.append({
                    'url': row['url'],
                    'title': parsed['title'],
                    'body_text': parsed['body_text'],
                    'word_count': parsed['word_count'],
                    'parse_success': parsed['parse_success']
                })
            else:
                results.append({
                    'url': row['url'],
                    'title': "",
                    'body_text': "",
                    'word_count': 0,
                    'parse_success': False
                })
        
        extracted_df = pd.DataFrame(results)
    
    # Filter successful parses
    print(f"\nSuccessfully parsed: {extracted_df['parse_success'].sum()} rows")
    print(f"Failed: {(~extracted_df['parse_success']).sum()} rows")
    
    # Save to CSV
    extracted_df.to_csv('data/extracted_content.csv', index=False)
    print("✓ Saved to data/extracted_content.csv")
    
    return extracted_df


# Process the dataset
# Note: Place your data.csv file in the data/ folder before running
extracted_df = process_dataset('data/data.csv')
print(f"\nDataset shape: {extracted_df.shape}")
print(extracted_df.head())

# ============================================================================
# SECTION 3: Text Preprocessing & Feature Engineering (25%)
# ============================================================================

def clean_text(text: str) -> str:
    """Clean and normalize text."""
    if not text:
        return ""
    # Lowercase and remove extra whitespace
    text = text.lower()
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def extract_keywords(text: str, top_n: int = 5) -> str:
    """Extract top N keywords using TF-IDF."""
    if not text:
        return ""
    
    try:
        # Simple TF-IDF for single document
        words = word_tokenize(text.lower())
        stop_words = set(stopwords.words('english'))
        words = [w for w in words if w.isalnum() and w not in stop_words and len(w) > 3]
        
        # Get top words by frequency (simplified TF)
        from collections import Counter
        word_freq = Counter(words)
        top_words = [word for word, _ in word_freq.most_common(top_n)]
        
        return '|'.join(top_words)
    except:
        return ""


def compute_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    Compute all features for the dataset.
    
    Args:
        df: DataFrame with extracted content
        
    Returns:
        DataFrame with computed features
    """
    print("Computing features...")
    
    # Filter valid rows
    df_valid = df[df['parse_success'] == True].copy()
    
    # Clean text
    df_valid['clean_text'] = df_valid['body_text'].apply(clean_text)
    
    # Basic metrics
    df_valid['sentence_count'] = df_valid['body_text'].apply(
        lambda x: len(sent_tokenize(x)) if x else 0
    )
    
    # Readability score (Flesch Reading Ease)
    df_valid['flesch_reading_ease'] = df_valid['body_text'].apply(
        lambda x: textstat.flesch_reading_ease(x) if x else 0
    )
    
    # Extract keywords
    print("Extracting keywords...")
    df_valid['top_keywords'] = df_valid['clean_text'].apply(
        lambda x: extract_keywords(x, top_n=5)
    )
    
    # Generate embeddings using sentence transformers
    print("Generating embeddings (this may take a minute)...")
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # Get embeddings for non-empty texts
    texts = df_valid['clean_text'].tolist()
    embeddings = model.encode(texts, show_progress_bar=True)
    
    # Convert embeddings to strings for CSV storage
    df_valid['embedding'] = [json.dumps(emb.tolist()) for emb in embeddings]
    
    # Also compute TF-IDF vectors for duplicate detection fallback
    tfidf = TfidfVectorizer(max_features=100, stop_words='english')
    tfidf_matrix = tfidf.fit_transform(texts)
    df_valid['tfidf_vector'] = [json.dumps(vec.toarray()[0].tolist()) for vec in tfidf_matrix]
    
    # Save features
    features_df = df_valid[[
        'url', 'title', 'word_count', 'sentence_count', 
        'flesch_reading_ease', 'top_keywords', 'embedding'
    ]].copy()
    
    features_df.to_csv('data/features.csv', index=False)
    print("✓ Saved to data/features.csv")
    
    return df_valid


# Compute features
features_df = compute_features(extracted_df)
print(f"\nFeatures computed for {len(features_df)} documents")
print(features_df[['url', 'word_count', 'sentence_count', 'flesch_reading_ease']].head())

# ============================================================================
# SECTION 4: Duplicate Detection (20%)
# ============================================================================

def detect_duplicates(df: pd.DataFrame, threshold: float = 0.80) -> pd.DataFrame:
    """
    Detect duplicate content using cosine similarity.
    
    Args:
        df: DataFrame with embeddings
        threshold: Similarity threshold (>threshold = duplicate)
        
    Returns:
        DataFrame with duplicate pairs
    """
    print(f"Detecting duplicates with threshold > {threshold}...")
    
    # Load embeddings
    embeddings = np.array([
        json.loads(emb) for emb in df['embedding']
    ])
    
    # Compute pairwise cosine similarity
    similarity_matrix = cosine_similarity(embeddings)
    
    # Find duplicate pairs
    duplicates = []
    n = len(df)
    
    for i in range(n):
        for j in range(i+1, n):
            sim = similarity_matrix[i, j]
            if sim > threshold:
                duplicates.append({
                    'url1': df.iloc[i]['url'],
                    'url2': df.iloc[j]['url'],
                    'similarity': round(sim, 3)
                })
    
    duplicates_df = pd.DataFrame(duplicates)
    
    # Detect thin content
    thin_content = df[df['word_count'] < 500].copy()
    df['is_thin'] = df['word_count'] < 500
    
    # Save results
    duplicates_df.to_csv('data/duplicates.csv', index=False)
    
    # Print summary
    print(f"\n{'='*60}")
    print(f"DUPLICATE DETECTION SUMMARY")
    print(f"{'='*60}")
    print(f"Total pages analyzed: {len(df)}")
    print(f"Duplicate pairs found: {len(duplicates_df)}")
    print(f"Thin content pages (< 500 words): {len(thin_content)} ({len(thin_content)/len(df)*100:.1f}%)")
    print(f"{'='*60}\n")
    
    if len(duplicates_df) > 0:
        print("Sample duplicate pairs:")
        print(duplicates_df.head())
    
    print("\n✓ Saved to data/duplicates.csv")
    
    return duplicates_df, df


# Detect duplicates
duplicates_df, features_df = detect_duplicates(features_df, threshold=0.80)

# ============================================================================
# SECTION 5: Content Quality Scoring (25%)
# ============================================================================

def create_quality_labels(df: pd.DataFrame) -> pd.DataFrame:
    """
    Create synthetic quality labels based on clear criteria.
    
    Labeling rules:
    - High: word_count > 1500 AND 50 <= readability <= 70
    - Low: word_count < 500 OR readability < 30
    - Medium: all other cases
    """
    df = df.copy()
    
    conditions = [
        (df['word_count'] > 1500) & (df['flesch_reading_ease'] >= 50) & (df['flesch_reading_ease'] <= 70),
        (df['word_count'] < 500) | (df['flesch_reading_ease'] < 30)
    ]
    
    choices = ['High', 'Low']
    
    df['quality_label'] = np.select(conditions, choices, default='Medium')
    
    print("Quality label distribution:")
    print(df['quality_label'].value_counts())
    
    return df


def train_quality_model(df: pd.DataFrame) -> Tuple:
    """
    Train a quality classification model.
    
    Returns:
        Trained model, feature names, metrics
    """
    print("\nTraining quality classification model...")
    
    # Prepare features
    feature_cols = ['word_count', 'sentence_count', 'flesch_reading_ease']
    X = df[feature_cols].values
    y = df['quality_label'].values
    
    # Train/test split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    print(f"Training set: {len(X_train)} samples")
    print(f"Test set: {len(X_test)} samples")
    
    # Train Random Forest
    model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)
    model.fit(X_train, y_train)
    
    # Predictions
    y_pred = model.predict(X_test)
    
    # Baseline: simple rule-based classifier (word count only)
    def baseline_predict(word_counts):
        preds = []
        for wc in word_counts:
            if wc > 1500:
                preds.append('High')
            elif wc < 500:
                preds.append('Low')
            else:
                preds.append('Medium')
        return np.array(preds)
    
    baseline_preds = baseline_predict(X_test[:, 0])
    
    # Metrics
    print(f"\n{'='*60}")
    print(f"MODEL PERFORMANCE")
    print(f"{'='*60}\n")
    
    print("Classification Report:")
    print(classification_report(y_test, y_pred, target_names=['High', 'Low', 'Medium']))
    
    print(f"\nOverall Accuracy: {accuracy_score(y_test, y_pred):.3f}")
    print(f"Baseline Accuracy: {accuracy_score(y_test, baseline_preds):.3f}")
    
    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_test, y_pred, labels=['High', 'Low', 'Medium'])
    print(cm)
    
    # Feature importance
    print("\nTop Features by Importance:")
    importances = model.feature_importances_
    for i, (feat, imp) in enumerate(sorted(zip(feature_cols, importances), key=lambda x: x[1], reverse=True)):
        print(f"{i+1}. {feat}: {imp:.3f}")
    
    print(f"{'='*60}\n")
    
    # Save model
    with open('models/quality_model.pkl', 'wb') as f:
        pickle.dump({
            'model': model,
            'feature_names': feature_cols
        }, f)
    
    print("✓ Model saved to models/quality_model.pkl")
    
    return model, feature_cols, {
        'accuracy': accuracy_score(y_test, y_pred),
        'baseline_accuracy': accuracy_score(y_test, baseline_preds)
    }


# Create labels and train model
features_df = create_quality_labels(features_df)
model, feature_names, metrics = train_quality_model(features_df)

# ============================================================================
# SECTION 6: Real-Time Analysis Demo (15%)
# ============================================================================

def analyze_url(url: str, reference_df: pd.DataFrame = None, model_path: str = 'models/quality_model.pkl') -> Dict:
    """
    Analyze a URL in real-time.
    
    Args:
        url: URL to analyze
        reference_df: DataFrame with existing content for duplicate detection
        model_path: Path to saved model
        
    Returns:
        Dictionary with analysis results
    """
    print(f"\nAnalyzing: {url}")
    
    try:
        # Scrape and parse
        html_content = scrape_url(url)
        if not html_content:
            return {"error": "Failed to scrape URL"}
        
        parsed = parse_html_content(html_content)
        
        # Extract features
        text = clean_text(parsed['body_text'])
        sentence_count = len(sent_tokenize(parsed['body_text'])) if parsed['body_text'] else 0
        flesch_score = textstat.flesch_reading_ease(parsed['body_text']) if parsed['body_text'] else 0
        
        # Load model
        with open(model_path, 'rb') as f:
            model_data = pickle.load(f)
            model = model_data['model']
            feature_names = model_data['feature_names']
        
        # Predict quality
        features = np.array([[parsed['word_count'], sentence_count, flesch_score]])
        quality_label = model.predict(features)[0]
        
        # Check if thin content
        is_thin = parsed['word_count'] < 500
        
        # Check for duplicates (if reference data provided)
        similar_to = []
        if reference_df is not None and len(reference_df) > 0:
            # Generate embedding for new content
            embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            new_embedding = embedding_model.encode([text])[0]
            
            # Compare with existing content
            existing_embeddings = np.array([
                json.loads(emb) for emb in reference_df['embedding']
            ])
            
            similarities = cosine_similarity([new_embedding], existing_embeddings)[0]
            
            # Find similar content (threshold 0.75 for reporting)
            for idx, sim in enumerate(similarities):
                if sim > 0.75:
                    similar_to.append({
                        'url': reference_df.iloc[idx]['url'],
                        'similarity': round(float(sim), 3)
                    })
        
        # Prepare result
        result = {
            'url': url,
            'title': parsed['title'],
            'word_count': parsed['word_count'],
            'sentence_count': sentence_count,
            'readability': round(flesch_score, 2),
            'quality_label': quality_label,
            'is_thin': is_thin,
            'similar_to': similar_to[:3]  # Top 3 similar
        }
        
        return result
    
    except Exception as e:
        return {"error": f"Analysis failed: {str(e)}"}


# Demo: Analyze a new URL
print("\n" + "="*60)
print("REAL-TIME URL ANALYSIS DEMO")
print("="*60)

# Example usage (replace with actual URL to test)
test_url = "https://example.com/article"  # Replace with actual URL

# Uncomment to test with a real URL:
# result = analyze_url(test_url, reference_df=features_df)
# print(json.dumps(result, indent=2))

print("\nTo analyze a URL, use:")
print("result = analyze_url('https://your-url-here.com', reference_df=features_df)")
print("print(json.dumps(result, indent=2))")

# ============================================================================
# FINAL SUMMARY
# ============================================================================

print("\n" + "="*70)
print("PIPELINE EXECUTION SUMMARY")
print("="*70)
print(f"✓ Data extracted and saved to: data/extracted_content.csv")
print(f"✓ Features computed and saved to: data/features.csv")
print(f"✓ Duplicates detected and saved to: data/duplicates.csv")
print(f"✓ Model trained and saved to: models/quality_model.pkl")
print(f"✓ Real-time analysis function: analyze_url() ready to use")
print("="*70)

print("\nKey Results:")
print(f"  • Total pages processed: {len(features_df)}")
print(f"  • Duplicate pairs found: {len(duplicates_df)}")
print(f"  • Model accuracy: {metrics['accuracy']:.3f}")
print(f"  • Baseline accuracy: {metrics['baseline_accuracy']:.3f}")
print(f"  • Improvement: {(metrics['accuracy'] - metrics['baseline_accuracy']):.3f}")

print("\n✅ Pipeline complete! All deliverables generated.")
